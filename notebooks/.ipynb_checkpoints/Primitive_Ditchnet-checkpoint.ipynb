{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41086dad-52ef-4036-a072-7c3adb3b74b0",
   "metadata": {},
   "source": [
    "# Primitive Unet on ditches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c08428f-be55-4fd8-bc7f-d9c55f0a8d60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 10:09:46.187151: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio # For images\n",
    "from skimage.transform import resize # For preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23754dff-a61f-4475-b925-e7982f927a66",
   "metadata": {},
   "source": [
    "## Read data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c703c1-4833-4d9a-b947-40fdd206f320",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:17: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "<timed exec>:21: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find a backend to open `/workspace/data/ditches/training/hpmf/Thumbs.db`` with iomode `ri`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:17\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/imageio/__init__.py:97\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"imread(uri, format=None, **kwargs)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03mReads an image from the specified file. Returns a numpy array, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    to see what arguments are available for a particular format.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting with ImageIO v3 the behavior of this function will switch to that of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m iio.v3.imread. To keep the current behavior (and make this warning disappear)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     95\u001b[0m )\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimread_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/imageio/v2.py:226\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m imopen_args \u001b[38;5;241m=\u001b[39m decypher_format_arg(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    224\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimopen_args\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    227\u001b[0m     result \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/imageio/core/imopen.py:298\u001b[0m, in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBased on the extension, the following plugins might add capable backends:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_candidates\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m         )\n\u001b[1;32m    297\u001b[0m request\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err_type(err_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find a backend to open `/workspace/data/ditches/training/hpmf/Thumbs.db`` with iomode `ri`."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "IMG_WIDTH = 512 # the original images are 500x500. is this a ok way to change size?\n",
    "IMG_HEIGHT = 512\n",
    "IMG_CHANNELS = 1 # a grey scale image only has one band for color.\n",
    "NUM_CLASSES = 1 # 0 = no crater and 1 = crater\n",
    "\n",
    "#### Training data ####\n",
    "TRAIN_PATH = '/workspace/data/ditches/training/'\n",
    "IMG_DIR = 'hpmf'\n",
    "GT_DIR = '3labels'\n",
    "X_train_ditches = []\n",
    "Y_train_ditches = []\n",
    "\n",
    "# load from disk\n",
    "img_path = os.path.join(TRAIN_PATH, IMG_DIR)\n",
    "gt_path = os.path.join(TRAIN_PATH, GT_DIR)\n",
    "for image in (os.listdir(img_path)):\n",
    "    if image.endswith('.tif'):\n",
    "        img = imageio.imread(os.path.join(img_path, image))\n",
    "\n",
    "        img = resize(img, (IMG_WIDTH, IMG_HEIGHT,1), mode='constant', preserve_range=True)\n",
    "\n",
    "        mask = imageio.imread(os.path.join(gt_path, image))\n",
    "        mask = resize(mask, (IMG_WIDTH, IMG_HEIGHT, 1), preserve_range=True, order=0).astype(int)\n",
    "\n",
    "        X_train_ditches.append(img)\n",
    "        Y_train_ditches.append(mask)\n",
    "\n",
    "#### Test data ####\n",
    "TEST_PATH = '/workspace/data/ditches/testing/'\n",
    "IMG_DIR = 'hpmf'\n",
    "GT_DIR = '3labels'\n",
    "X_test_ditches = []\n",
    "Y_test_ditches = []\n",
    "\n",
    "# load from disk\n",
    "img_path = os.path.join(TEST_PATH, IMG_DIR)\n",
    "gt_path = os.path.join(TEST_PATH, GT_DIR)\n",
    "for image in (os.listdir(img_path)):\n",
    "    if image.endswith('.tif'):\n",
    "        img = imageio.imread(os.path.join(img_path, image))\n",
    "        img = resize(img, (IMG_WIDTH, IMG_HEIGHT,1), mode='constant', preserve_range=True)\n",
    "        mask = imageio.imread(os.path.join(gt_path, image))\n",
    "        mask = resize(mask, (IMG_WIDTH, IMG_HEIGHT, 1), preserve_range=True, order=0).astype(int)\n",
    "\n",
    "        X_test_ditches.append(img)\n",
    "        Y_test_ditches.append(mask)\n",
    "\n",
    "# convert list of numpy arrays into tensorflow dataset for further processing\n",
    "train_images_ditches = tf.data.Dataset.from_tensor_slices((X_train_ditches, Y_train_ditches))\n",
    "test_images_ditches = tf.data.Dataset.from_tensor_slices((X_test_ditches, Y_test_ditches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de604e91-4af7-426e-a5af-baf185845c23",
   "metadata": {},
   "source": [
    "## Set up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28a9586-8883-4f21-a88d-df106f8c5c54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_images_ditches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m BUFFER_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_batches_ditches  \u001b[38;5;241m=\u001b[39m (\u001b[43mtrain_images_ditches\u001b[49m\n\u001b[1;32m      6\u001b[0m                     \u001b[38;5;241m.\u001b[39mcache() \u001b[38;5;66;03m# cache data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m                     \u001b[38;5;241m.\u001b[39mshuffle(BUFFER_SIZE) \u001b[38;5;66;03m# fill buffer, sample from it and replace with new items (buffer size > training set for perfect shuffling)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                     \u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE)  \n\u001b[1;32m      9\u001b[0m                     \u001b[38;5;241m.\u001b[39mrepeat() \n\u001b[1;32m     10\u001b[0m                     \u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# testing\u001b[39;00m\n\u001b[1;32m     12\u001b[0m test_batches_ditches  \u001b[38;5;241m=\u001b[39m (test_images_ditches \n\u001b[1;32m     13\u001b[0m                     \u001b[38;5;241m.\u001b[39mcache() \u001b[38;5;66;03m# cache data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m                     \u001b[38;5;241m.\u001b[39mshuffle(BUFFER_SIZE) \u001b[38;5;66;03m# fill buffer, sample from it and replace with new items (buffer size > training set for perfect shuffling)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m                     \u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE)  \n\u001b[1;32m     16\u001b[0m                     \u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# repeat dataset idefinetely\u001b[39;00m\n\u001b[1;32m     17\u001b[0m                     \u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_images_ditches' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 128\n",
    "\n",
    "# training\n",
    "train_batches_ditches  = (train_images_ditches\n",
    "                    .cache() # cache data\n",
    "                    .shuffle(BUFFER_SIZE) # fill buffer, sample from it and replace with new items (buffer size > training set for perfect shuffling)\n",
    "                    .batch(BATCH_SIZE)  \n",
    "                    .repeat() \n",
    "                    .prefetch(buffer_size=128))\n",
    "# testing\n",
    "test_batches_ditches  = (test_images_ditches \n",
    "                    .cache() # cache data\n",
    "                    .shuffle(BUFFER_SIZE) # fill buffer, sample from it and replace with new items (buffer size > training set for perfect shuffling)\n",
    "                    .batch(BATCH_SIZE)  \n",
    "                    .repeat(1)  # repeat dataset idefinetely\n",
    "                    .prefetch(buffer_size=128)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992491c-7fbc-453c-a6fe-e2f930f40724",
   "metadata": {},
   "source": [
    "## Add f1-score as a metric to monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef778db-eae4-4a0e-be14-d379d451a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8545d-82ce-4714-b4cd-da56323fc13f",
   "metadata": {},
   "source": [
    "## Basic U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce01db-b69d-484a-b1b5-3f5bcefb3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "NUM_CLASSES = 1\n",
    "#Contraction path\n",
    "c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "c1 = tf.keras.layers.Dropout(0.1)(c1) # to prevent overfitting\n",
    "c1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "c2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
    " \n",
    "c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
    "c3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
    " \n",
    "c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
    "c4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
    " \n",
    "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
    "c5 = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "#Expansive path \n",
    "u6 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
    "c6 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    " \n",
    "u7 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
    "c7 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    " \n",
    "u8 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "c8 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    " \n",
    "u9 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "c9 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    " \n",
    "outputs = tf.keras.layers.Conv2D(NUM_CLASSES, (1, 1))(c9)\n",
    "\n",
    "model_ditches = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "model_ditches.compile(optimizer='adam', loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True), metrics=['acc', f1_m, recall_m])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893578a-e792-4ce3-8c45-f0a95ad32096",
   "metadata": {},
   "source": [
    "## Set Weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ee054-aca0-44fe-b567-96ca0c37e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sample_weights(image, label):\n",
    "    # The weights for each class, with the constraint that:\n",
    "    #     sum(class_weights) == 1.0\n",
    "    class_weights = tf.constant([0.01, 1]) # the first weight is for the background and the second for the craters.\n",
    "    class_weights = class_weights/tf.reduce_sum(class_weights)\n",
    "\n",
    "    # Create an image of `sample_weights` by using the label at each pixel as an \n",
    "    # index into the `class weights` .\n",
    "    sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
    "\n",
    "    return image, label, sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3936889-ce52-4858-94bf-f7183835a13f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56848024-04c8-48c1-b00a-b0b6dbac8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_ditches = model_ditches.fit(train_batches_ditches.map(add_sample_weights), epochs=200, steps_per_epoch=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b215a8-2d0b-4f7f-91c2-7702a004ac3e",
   "metadata": {},
   "source": [
    "## Plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d67a8-0d3e-49b5-87f0-1e0c33f76c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(result_ditches.history['loss'])\n",
    "plt.title('Model accuracy using f1 score')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55512cbd-38d0-4d0a-81b0-e0061803d9b8",
   "metadata": {},
   "source": [
    "## Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416c10a-d999-44ba-8286-db63a2adaf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model__ditches.evaluate(test_batches_ditches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
